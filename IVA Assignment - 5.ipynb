{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdcae9-6409-443d-a60a-1ab6c1115c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def calculate_motion_mask(previous_frame, current_frame):\n",
    "    # Ensure both frames are in BGR color space\n",
    "    bgr_previous = cv2.cvtColor(previous_frame, cv2.COLOR_RGB2BGR)\n",
    "    bgr_current = cv2.cvtColor(current_frame, cv2.COLOR_RGB2BGR)\n",
    "   \n",
    "    # Convert frames to grayscale\n",
    "    gray_prev = cv2.cvtColor(bgr_previous, cv2.COLOR_BGR2GRAY)\n",
    "    gray_curr = cv2.cvtColor(bgr_current, cv2.COLOR_BGR2GRAY)\n",
    "   \n",
    "    # Calculate frame difference\n",
    "    frame_diff = cv2.absdiff(gray_prev, gray_curr)\n",
    "   \n",
    "    # Apply threshold to get motion mask\n",
    "    _, motion_mask = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)\n",
    "   \n",
    "    return motion_mask\n",
    "\n",
    "def detect_events(motion_mask, frame):\n",
    "    contours, _ = cv2.findContours(motion_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area > 1000:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"Event Detected\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            timestamp = int(cv2.getTickCount() / cv2.getTickFrequency())\n",
    "            cv2.imwrite(f\"event_{timestamp}.jpg\", frame)\n",
    "\n",
    "video_path = \"iva.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "previous_frame = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "   \n",
    "    if previous_frame is None:\n",
    "        previous_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        continue\n",
    "   \n",
    "    # Calculate motion mask\n",
    "    motion_mask = calculate_motion_mask(previous_frame, cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "   \n",
    "    # Update previous frame\n",
    "    previous_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "   \n",
    "    # Detect events\n",
    "    detect_events(motion_mask, frame)\n",
    "   \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Motion Detection', frame)\n",
    "   \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0c6da3-f69b-4316-b54c-b8306ab38142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1...\n",
      "Overall Image 1 sentiment: neutral\n",
      "Processing image 2...\n",
      "Overall Image 2 sentiment: happy\n",
      "Processing image 3...\n",
      "Overall Image 3 sentiment: happy\n",
      "Processing image 4...\n",
      "Overall Image 4 sentiment: sad\n",
      "Processing image 5...\n",
      "Overall Image 5 sentiment: happy\n",
      "Processing image 6...\n",
      "Overall Image 6 sentiment: sad\n",
      "Processing image 7...\n",
      "Overall Image 7 sentiment: sad\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import numpy as np\n",
    "\n",
    "def load_image_set(image_paths):\n",
    "    \"\"\"Load a set of images.\"\"\"\n",
    "    return [cv2.imread(path) for path in image_paths]\n",
    "\n",
    "def detect_faces(image):\n",
    "    \"\"\"Detect faces using Haar cascade face detection.\"\"\"\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    return faces\n",
    "\n",
    "def classify_emotion(face_image):\n",
    "    \"\"\"Classify emotion using a pre-trained FER model.\"\"\"\n",
    "    emotion_detector = FER(mtcnn=True)\n",
    "    result = emotion_detector.detect_emotions(face_image)\n",
    "    \n",
    "    if result:\n",
    "        # Return the emotion with the highest confidence\n",
    "        dominant_emotion = max(result[0]['emotions'], key=result[0]['emotions'].get)\n",
    "        return dominant_emotion\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "def categorize_image_sentiment(individual_sentiments):\n",
    "    \"\"\"Categorize overall sentiment of the image.\"\"\"\n",
    "    happy_count = sum(1 for s in individual_sentiments if s == 'happy')\n",
    "    sad_count = sum(1 for s in individual_sentiments if s == 'sad')\n",
    "    \n",
    "    if happy_count > sad_count:\n",
    "        return 'happy'\n",
    "    elif sad_count > happy_count:\n",
    "        return 'sad'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def analyze_crowd(image_paths):\n",
    "    \"\"\"Analyze crowd sentiment.\"\"\"\n",
    "    images = load_image_set(image_paths)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        print(f\"Processing image {i+1}...\")\n",
    "        \n",
    "        faces = detect_faces(image)\n",
    "        individual_sentiments = []\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            face_image = image[y:y+h, x:x+w]\n",
    "            emotion = classify_emotion(face_image)\n",
    "            individual_sentiments.append(emotion)\n",
    "            \n",
    "            # Display sentiments and facial features\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            cv2.putText(image, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        overall_sentiment = categorize_image_sentiment(individual_sentiments)\n",
    "        print(f\"Overall Image {i+1} sentiment: {overall_sentiment}\")\n",
    "        \n",
    "        cv2.imshow(f\"Image {i+1}\", image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "image_paths = ['img1.jpeg', 'img2.jpeg', 'img3.jpeg', 'img4.jpeg', 'img5.jpeg', 'img6.jpeg', 'img7.jpg']\n",
    "analyze_crowd(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68a69525-2414-45f3-a23f-6febc426d424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detections for image 1:\n",
      "\n",
      "Final detections:\n",
      "Face 1:\n",
      "  Position: (22, 11) - (73, 62)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (304, 11) - (360, 67)\n",
      "  Size: Width=56, Height=56\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (304, 11) - (360, 67)\n",
      "  Size: Width=56, Height=56\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (121, 100) - (172, 151)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (121, 100) - (172, 151)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (114, 197) - (173, 256)\n",
      "  Size: Width=59, Height=59\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (114, 197) - (173, 256)\n",
      "  Size: Width=59, Height=59\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (400, 197) - (454, 251)\n",
      "  Size: Width=54, Height=54\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (400, 197) - (454, 251)\n",
      "  Size: Width=54, Height=54\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (115, 8) - (173, 66)\n",
      "  Size: Width=58, Height=58\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (115, 8) - (173, 66)\n",
      "  Size: Width=58, Height=58\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (304, 102) - (360, 158)\n",
      "  Size: Width=56, Height=56\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (304, 102) - (360, 158)\n",
      "  Size: Width=56, Height=56\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (20, 108) - (79, 167)\n",
      "  Size: Width=59, Height=59\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (20, 108) - (79, 167)\n",
      "  Size: Width=59, Height=59\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (208, 201) - (269, 262)\n",
      "  Size: Width=61, Height=61\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (208, 201) - (269, 262)\n",
      "  Size: Width=61, Height=61\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (394, 11) - (455, 72)\n",
      "  Size: Width=61, Height=61\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (394, 11) - (455, 72)\n",
      "  Size: Width=61, Height=61\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (398, 102) - (449, 153)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (398, 102) - (449, 153)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (202, 103) - (274, 175)\n",
      "  Size: Width=72, Height=72\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (202, 103) - (274, 175)\n",
      "  Size: Width=72, Height=72\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (119, 294) - (169, 344)\n",
      "  Size: Width=50, Height=50\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (119, 294) - (169, 344)\n",
      "  Size: Width=50, Height=50\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (27, 296) - (75, 344)\n",
      "  Size: Width=48, Height=48\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (27, 296) - (75, 344)\n",
      "  Size: Width=48, Height=48\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (310, 299) - (361, 350)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (310, 299) - (361, 350)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (21, 387) - (79, 445)\n",
      "  Size: Width=58, Height=58\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (21, 387) - (79, 445)\n",
      "  Size: Width=58, Height=58\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (117, 384) - (171, 438)\n",
      "  Size: Width=54, Height=54\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (117, 384) - (171, 438)\n",
      "  Size: Width=54, Height=54\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (309, 396) - (355, 442)\n",
      "  Size: Width=46, Height=46\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (309, 396) - (355, 442)\n",
      "  Size: Width=46, Height=46\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (397, 389) - (458, 450)\n",
      "  Size: Width=61, Height=61\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (397, 389) - (458, 450)\n",
      "  Size: Width=61, Height=61\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (400, 296) - (454, 350)\n",
      "  Size: Width=54, Height=54\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (400, 296) - (454, 350)\n",
      "  Size: Width=54, Height=54\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (87, 350) - (202, 465)\n",
      "  Size: Width=115, Height=115\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (87, 350) - (202, 465)\n",
      "  Size: Width=115, Height=115\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (123, 97) - (288, 255)\n",
      "  Size: Width=165, Height=158\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (123, 97) - (288, 255)\n",
      "  Size: Width=165, Height=158\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (26, 107) - (99, 276)\n",
      "  Size: Width=73, Height=169\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (26, 107) - (99, 276)\n",
      "  Size: Width=73, Height=169\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (27, 9) - (100, 74)\n",
      "  Size: Width=73, Height=65\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (27, 9) - (100, 74)\n",
      "  Size: Width=73, Height=65\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (121, 6) - (285, 84)\n",
      "  Size: Width=164, Height=78\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 1:\n",
      "  Position: (121, 6) - (285, 84)\n",
      "  Size: Width=164, Height=78\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "\n",
      "Detections for image 2:\n",
      "\n",
      "Final detections:\n",
      "Face 2:\n",
      "  Position: (128, 34) - (225, 131)\n",
      "  Size: Width=97, Height=97\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 2:\n",
      "  Position: (128, 34) - (225, 131)\n",
      "  Size: Width=97, Height=97\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "\n",
      "Detections for image 3:\n",
      "\n",
      "Final detections:\n",
      "Face 3:\n",
      "  Position: (116, 44) - (214, 142)\n",
      "  Size: Width=98, Height=98\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "Face 3:\n",
      "  Position: (116, 44) - (214, 142)\n",
      "  Size: Width=98, Height=98\n",
      "  Confidence: 0.80\n",
      "  Gender: Female\n",
      "---\n",
      "\n",
      "Detections for image 4:\n",
      "\n",
      "Final detections:\n",
      "Face 4:\n",
      "  Position: (200, 190) - (264, 254)\n",
      "  Size: Width=64, Height=64\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (200, 190) - (264, 254)\n",
      "  Size: Width=64, Height=64\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (215, 69) - (266, 120)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (215, 69) - (266, 120)\n",
      "  Size: Width=51, Height=51\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (279, 139) - (327, 187)\n",
      "  Size: Width=48, Height=48\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (279, 139) - (327, 187)\n",
      "  Size: Width=48, Height=48\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (28, 143) - (81, 196)\n",
      "  Size: Width=53, Height=53\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (28, 143) - (81, 196)\n",
      "  Size: Width=53, Height=53\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (130, 133) - (185, 188)\n",
      "  Size: Width=55, Height=55\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 4:\n",
      "  Position: (130, 133) - (185, 188)\n",
      "  Size: Width=55, Height=55\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "\n",
      "Detections for image 5:\n",
      "\n",
      "Final detections:\n",
      "Face 5:\n",
      "  Position: (138, 115) - (206, 183)\n",
      "  Size: Width=68, Height=68\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (138, 115) - (206, 183)\n",
      "  Size: Width=68, Height=68\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (107, 46) - (165, 104)\n",
      "  Size: Width=58, Height=58\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (107, 46) - (165, 104)\n",
      "  Size: Width=58, Height=58\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (270, 76) - (333, 139)\n",
      "  Size: Width=63, Height=63\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (270, 76) - (333, 139)\n",
      "  Size: Width=63, Height=63\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (187, 66) - (252, 131)\n",
      "  Size: Width=65, Height=65\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (187, 66) - (252, 131)\n",
      "  Size: Width=65, Height=65\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (52, 86) - (112, 146)\n",
      "  Size: Width=60, Height=60\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (52, 86) - (112, 146)\n",
      "  Size: Width=60, Height=60\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (344, 76) - (417, 149)\n",
      "  Size: Width=73, Height=73\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (344, 76) - (417, 149)\n",
      "  Size: Width=73, Height=73\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (51, 82) - (158, 228)\n",
      "  Size: Width=107, Height=146\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (51, 82) - (158, 228)\n",
      "  Size: Width=107, Height=146\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (113, 48) - (272, 165)\n",
      "  Size: Width=159, Height=117\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 5:\n",
      "  Position: (113, 48) - (272, 165)\n",
      "  Size: Width=159, Height=117\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "\n",
      "Detections for image 6:\n",
      "\n",
      "Final detections:\n",
      "Face 6:\n",
      "  Position: (214, 1169) - (527, 1482)\n",
      "  Size: Width=313, Height=313\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (214, 1169) - (527, 1482)\n",
      "  Size: Width=313, Height=313\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (962, 1242) - (1203, 1483)\n",
      "  Size: Width=241, Height=241\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (962, 1242) - (1203, 1483)\n",
      "  Size: Width=241, Height=241\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1994, 141) - (2096, 243)\n",
      "  Size: Width=102, Height=102\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1994, 141) - (2096, 243)\n",
      "  Size: Width=102, Height=102\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1199, 22) - (1306, 129)\n",
      "  Size: Width=107, Height=107\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1199, 22) - (1306, 129)\n",
      "  Size: Width=107, Height=107\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (182, 196) - (295, 309)\n",
      "  Size: Width=113, Height=113\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (182, 196) - (295, 309)\n",
      "  Size: Width=113, Height=113\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1022, 828) - (1219, 1025)\n",
      "  Size: Width=197, Height=197\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1022, 828) - (1219, 1025)\n",
      "  Size: Width=197, Height=197\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (405, 260) - (530, 385)\n",
      "  Size: Width=125, Height=125\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (405, 260) - (530, 385)\n",
      "  Size: Width=125, Height=125\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1607, 283) - (1747, 423)\n",
      "  Size: Width=140, Height=140\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1607, 283) - (1747, 423)\n",
      "  Size: Width=140, Height=140\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1714, 995) - (1949, 1230)\n",
      "  Size: Width=235, Height=235\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1714, 995) - (1949, 1230)\n",
      "  Size: Width=235, Height=235\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (478, 836) - (743, 1101)\n",
      "  Size: Width=265, Height=265\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (478, 836) - (743, 1101)\n",
      "  Size: Width=265, Height=265\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1500, 915) - (1739, 1154)\n",
      "  Size: Width=239, Height=239\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1500, 915) - (1739, 1154)\n",
      "  Size: Width=239, Height=239\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1422, 610) - (1622, 810)\n",
      "  Size: Width=200, Height=200\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1422, 610) - (1622, 810)\n",
      "  Size: Width=200, Height=200\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (83, 687) - (273, 877)\n",
      "  Size: Width=190, Height=190\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (83, 687) - (273, 877)\n",
      "  Size: Width=190, Height=190\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (292, 922) - (480, 1110)\n",
      "  Size: Width=188, Height=188\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (292, 922) - (480, 1110)\n",
      "  Size: Width=188, Height=188\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (683, 656) - (881, 854)\n",
      "  Size: Width=198, Height=198\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (683, 656) - (881, 854)\n",
      "  Size: Width=198, Height=198\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1212, 363) - (1341, 492)\n",
      "  Size: Width=129, Height=129\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1212, 363) - (1341, 492)\n",
      "  Size: Width=129, Height=129\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (722, 56) - (825, 159)\n",
      "  Size: Width=103, Height=103\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (722, 56) - (825, 159)\n",
      "  Size: Width=103, Height=103\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1807, 292) - (1982, 467)\n",
      "  Size: Width=175, Height=175\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1807, 292) - (1982, 467)\n",
      "  Size: Width=175, Height=175\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (722, 249) - (836, 363)\n",
      "  Size: Width=114, Height=114\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (722, 249) - (836, 363)\n",
      "  Size: Width=114, Height=114\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (182, 485) - (358, 661)\n",
      "  Size: Width=176, Height=176\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (182, 485) - (358, 661)\n",
      "  Size: Width=176, Height=176\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (798, 353) - (953, 508)\n",
      "  Size: Width=155, Height=155\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (798, 353) - (953, 508)\n",
      "  Size: Width=155, Height=155\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1442, 354) - (1600, 512)\n",
      "  Size: Width=158, Height=158\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (1442, 354) - (1600, 512)\n",
      "  Size: Width=158, Height=158\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (631, 337) - (804, 510)\n",
      "  Size: Width=173, Height=173\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (631, 337) - (804, 510)\n",
      "  Size: Width=173, Height=173\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (634, 343) - (1416, 863)\n",
      "  Size: Width=782, Height=520\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (178, 472) - (524, 1132)\n",
      "  Size: Width=346, Height=660\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (178, 472) - (524, 1132)\n",
      "  Size: Width=346, Height=660\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (825, 349) - (1771, 871)\n",
      "  Size: Width=946, Height=522\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (825, 349) - (1771, 871)\n",
      "  Size: Width=946, Height=522\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (411, 250) - (938, 651)\n",
      "  Size: Width=527, Height=401\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n",
      "Face 6:\n",
      "  Position: (411, 250) - (938, 651)\n",
      "  Size: Width=527, Height=401\n",
      "  Confidence: 0.80\n",
      "  Gender: Male\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load OpenCV's deep learning face detector\n",
    "net = cv2.dnn.readNetFromCaffe('deploy.prototxt.txt', 'res10_300x300_ssd_iter_140000.caffemodel')\n",
    "\n",
    "# Define MODEL_MEAN_VALUES\n",
    "MODEL_MEAN_VALUES = (78.42633776026858, 102.15891835115144, 117.08628437568004)\n",
    "\n",
    "# Load gender detection model\n",
    "gender_prototxt = 'gender_deploy.prototxt'\n",
    "gender_model = 'gender_net.caffemodel'\n",
    "gender_net = cv2.dnn.readNet(gender_prototxt, gender_model)\n",
    "\n",
    "# Define gender classes\n",
    "gender_classes = ['Male', 'Female']\n",
    "\n",
    "def detect_faces(image):\n",
    "    # Detect faces using Haar cascade\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces_haar = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=6, minSize=(30, 30))\n",
    "\n",
    "    # Detect faces using deep learning\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104.0, 177.0, 123.0])\n",
    "    net.setInput(blob)\n",
    "    faces_dnn = net.forward()\n",
    "\n",
    "    # Combine results and apply strict criteria\n",
    "    combined_results = []\n",
    "    for (x, y, w, h) in faces_haar:\n",
    "        if w < image.shape[1] // 2 and h < image.shape[0] // 2:  # Check if face is within half the image size\n",
    "            combined_results.append((x, y, w, h, 0.8))  # Default confidence for Haar cascade\n",
    "    for i in range(faces_dnn.shape[2]):\n",
    "        confidence = faces_dnn[0, 0, i, 2]\n",
    "        if confidence > 0.7:  # Stricter confidence threshold\n",
    "            x = int(faces_dnn[0, 0, i, 3] * image.shape[1])\n",
    "            y = int(faces_dnn[0, 0, i, 4] * image.shape[0])\n",
    "            w = int(faces_dnn[0, 0, i, 5] * image.shape[1])\n",
    "            h = int(faces_dnn[0, 0, i, 6] * image.shape[0])\n",
    "            if w < image.shape[1] // 2 and h < image.shape[0] // 2:  # Check if face is within half the image size\n",
    "                combined_results.append((x, y, w, h, confidence))\n",
    "\n",
    "    # Apply non-maximum suppression twice\n",
    "    def nms_boxes(detections, threshold):\n",
    "        if len(detections) > 0:\n",
    "            detections = sorted(detections, key=lambda x: x[4], reverse=True)\n",
    "            suppressed = []\n",
    "            for i in range(len(detections)):\n",
    "                if detections[i][4] > threshold:\n",
    "                    suppressed.append(detections[i])\n",
    "                    for j in range(i+1, len(detections)):\n",
    "                        if iou(detections[i][:4], detections[j][:4]) > threshold:\n",
    "                            break\n",
    "                    else:\n",
    "                        suppressed.append(detections[i])\n",
    "            return suppressed\n",
    "        return []\n",
    "\n",
    "    def iou(box_a, box_b):\n",
    "        x1, y1, w1, h1 = box_a\n",
    "        x2, y2, w2, h2 = box_b\n",
    "        intersection_area = max(0, min(x1+w1, x2+w2)-max(x1,x2)) * max(0, min(y1+h1, y2+h2)-max(y1,y2))\n",
    "        union_area = float(w1*h1 + w2*h2 - intersection_area)\n",
    "        return intersection_area / union_area\n",
    "\n",
    "    # First pass: NMS for Haar cascade detections\n",
    "    haarsuppressed = nms_boxes([(x, y, w, h, 0.8) for x, y, w, h, _ in combined_results], 0.3)\n",
    "\n",
    "    # Second pass: NMS for deep learning detections\n",
    "    dnnsuppressed = nms_boxes([(x, y, w, h, confidence) for x, y, w, h, confidence in combined_results], 0.3)\n",
    "\n",
    "    # Combine results, keeping only detections that appear in both methods\n",
    "    final_detections = []\n",
    "    for x, y, w, h, _ in haarsuppressed:\n",
    "        if any(x <= det_x < x+w and y <= det_y < y+h and x+w > det_x and y+h > det_y for det_x, det_y, _, _, _ in dnnsuppressed):\n",
    "            final_detections.append((x, y, w, h, 0.8))  # Using Haar cascade confidence as default\n",
    "\n",
    "    return final_detections\n",
    "\n",
    "def detect_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "    gender_net.setInput(blob)\n",
    "    gender_preds = gender_net.forward()\n",
    "    gender = gender_classes[gender_preds[0].argmax()]\n",
    "    return gender\n",
    "\n",
    "def get_optimal_font_scale(text, width, height):\n",
    "    for scale in reversed(range(0, 60, 1)):\n",
    "        textSize = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, scale/10, 2)\n",
    "        new_width = textSize[0][0]\n",
    "        new_height = textSize[0][1]\n",
    "        if (new_width <= width*0.95 and new_height <= height*0.95):\n",
    "            return scale/10\n",
    "    return 1\n",
    "\n",
    "def process_images(image_paths):\n",
    "    for image_path in image_paths:\n",
    "        try:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Error: Could not load image {image_path}\")\n",
    "                continue\n",
    "            \n",
    "            faces = detect_faces(image)\n",
    "\n",
    "            print(f\"\\nDetections for image {image_paths.index(image_path)+1}:\")\n",
    "            for face in faces:\n",
    "                x, y, w, h, confidence = face\n",
    "                cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                \n",
    "                # Extract face region\n",
    "                face_region = image[y:y+h, x:x+w]\n",
    "                \n",
    "                # Detect gender\n",
    "                gender = detect_gender(face_region)\n",
    "\n",
    "                # Calculate optimal font scale\n",
    "                text_width = image.shape[1] * 0.8\n",
    "                text_height = image.shape[0] * 0.05\n",
    "                font_scale = get_optimal_font_scale(f\"Face {image_paths.index(image_path)+1}\", text_width, text_height)\n",
    "                \n",
    "                # Display text\n",
    "                cv2.putText(image, f\"{gender}\", (x, y+h+30), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.imshow(\"Faces\", image)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            print(\"\\nFinal detections:\")\n",
    "            for face in faces:\n",
    "                x, y, w, h, confidence = face\n",
    "                print(f\"Face {image_paths.index(image_path)+1}:\")\n",
    "                print(f\"  Position: ({x}, {y}) - ({x+w}, {y+h})\")\n",
    "                print(f\"  Size: Width={w}, Height={h}\")\n",
    "                print(f\"  Confidence: {confidence:.2f}\")\n",
    "                print(f\"  Gender: {gender}\")\n",
    "                print(\"---\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "\n",
    "# Test the improved detection on all images\n",
    "process_images(['img2.jpeg', 'img3.jpeg', 'img4.jpeg', 'img5.jpeg', 'img6.jpeg', 'img7.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66538e61-1ec7-4e47-92d7-4f9ccc036abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
